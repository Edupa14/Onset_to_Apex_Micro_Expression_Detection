{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score,f1_score\n", "from keras.models import Sequential, Model\n", "from keras.layers.core import Dense, Dropout, Activation, Flatten\n", "from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D\n", "from keras.layers import LeakyReLU ,PReLU,BatchNormalization,concatenate,Input\n", "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,Callback\n", "from sklearn.model_selection import train_test_split,LeaveOneOut,KFold\n", "from keras import backend as K\n", "from keras.optimizers import Adam,SGD\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class myCallback(Callback):\n", "    def on_epoch_end(self, epoch, logs={}):\n", "        if(logs.get('val_acc') >= 1.0):\n", "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(1.0*100))\n", "            self.model.stop_training = True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(segment_train_images, segment_validation_images, segment_train_labels, segment_validation_labels,test_index,segment_train_images_cat ,segment_validation_images_cat):\n", "    layer_in = Input(shape=(1, sizeH, sizeV, sizeD))\n", "    # conv1 = Convolution3D(256, (20, 20, 9), strides=(10, 10, 3), padding='Same')(input)\n", "    # # bn1=BatchNormalization()(conv1)\n", "    # ract_1 = PReLU()(conv1)\n", "    conv1 = Convolution3D(96, (20, 20, 1), strides=(10, 10, 1), padding='same')(layer_in)\n", "    ract_211 = PReLU()(conv1)\n", "    # 3x3 conv\n", "    conv3 = Convolution3D(256, (20, 20, 1), strides=(10, 10, 1), padding='same')(layer_in)\n", "    conv3 = Convolution3D(512, (3, 3, 1), padding='same')(conv3)\n", "    ract_212 = PReLU()(conv3)\n", "    # 5x5 conv\n", "    # conv5 = Convolution3D(16, (20, 20, 1), strides=(10, 10, 1), padding='same', activation='relu')(layer_in)\n", "    # conv5 = Convolution3D(32, (5, 5, 1), padding='same', activation='relu')(conv5)\n", "    # 3x3 max pooling\n", "    pool = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same')(layer_in)\n", "    pool = Convolution3D(32, (20, 20, 1), strides=(10, 10, 1), padding='same')(pool)\n", "    ract_213 = PReLU()(pool)\n", "    # concatenate filters, assumes filters/channels last\n", "    layer_out = concatenate([ract_211, ract_212,  ract_213], axis=-4)\n", "    # add1= Add() ([conv3,ract_1])\n", "    # drop0 = Dropout(0.5)(layer_out)\n", "    # conv6 = Convolution3D(512, (3, 3, 3), strides=1, padding='Same')(drop0)\n", "    # # bn3 = BatchNormalization()(conv3)\n", "    ract_4 = PReLU()(layer_out)\n", "    flatten_1 = Flatten()(ract_4)\n", "    # dense_1 = Dense(1024, init='normal')(flatten_1)\n", "    # dense_2 = Dense(128, init='normal')(dense_1)\n", "    layer_in2 = Input(shape=(1, sizeH2, sizeV2, sizeD2))\n", "    conv21 = Convolution3D(32, (20, 20, 30), strides=(10, 10, 15), padding='Same')(layer_in2)\n", "    ract_21 = PReLU()(conv21)\n", "    conv22 = Convolution3D(32, (3, 3, 3), strides=1, padding='Same')(ract_21)\n", "    ract_22 = PReLU()(conv22)\n", "    flatten_2 = Flatten()(ract_22)\n", "    flatten_3 = Flatten()(layer_in2)\n", "    # drop11 = Dropout(0.8)(flatten_1)\n", "    # drop21 = Dropout(0.8)(flatten_2)\n", "    # drop31 = Dropout(0.8)(flatten_3)\n", "    concat = concatenate([flatten_1, flatten_2, flatten_3], axis=-1)\n", "    drop51 = Dropout(0.5)(concat)\n", "    dense_3 = Dense(5, init='normal')(drop51)\n", "    # drop1 = Dropout(0.5)(dense_3)\n", "    activation = Activation('softmax')(dense_3)\n", "    opt = SGD(lr=0.01)\n", "    model = Model(inputs=[layer_in,layer_in2], outputs=activation)\n", "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n", "# ----------------------------\n", "    #     model = Sequential()`\n", "    #     # model.add(ZeroPadding3D((2,2,0)))\n", "    #     model.add(Convolution3D(32, (6, 6, 1), strides=(3, 3, 1), input_shape=(1, sizeH, sizeV, sizeD), padding='Same'))\n", "    #\n", "    #     model.add(Convolution3D(64, (12, 12, 1), strides=(6, 6, 1), input_shape=(1, sizeH, sizeV, sizeD), padding='Same'))\n", "    #     model.add(PReLU())`\n", "#     # model.add(Convolution3D(128, (8, 8, 1), strides=1, input_shape=(1, sizeH, sizeV, sizeD), padding='Same'))\n", "#     # model.add(PReLU())\n", "#     # model.add(Dropout(0.5))\n", "#     # 3\n", "#     # model.add(Convolution3D(32, (3, 3, 2), strides=1, padding='Same'))\n", "#     # model.add(PReLU())\n", "#     # 40\n", "#     # model.add(Dropout(0.5))\n", "#     # 1\n", "#     model.add(MaxPooling3D(pool_size=(3, 3, 2)))\n", "#     model.add(PReLU())\n", "#     # 2\n", "#     # model.add(Dropout(0.5))\n", "#     model.add(Flatten())\n", "#     model.add(Dense(256, init='normal'))\n", "#     # model.add(Dropout(0.5))\n", "#     model.add(Dense(128, init='normal'))\n", "#     # model.add(PReLU())\n", "#     # model.add(Dense(128, init='normal'))`\n", "#     model.add(Dropout(0.5))\n", "#     model.add(Dense(5, init='normal'))\n", "#     # model.add(Dropout(0.5))\n", "#     model.add(Activation('softmax'))\n", "#     opt = SGD(lr=0.1)\n", "#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n", "    model.summary()\n", "    filepath = \"weights_CAS(ME)2/weights-improvement\" + str(test_index) + \"-{epoch:02d}-{val_acc:.2f}.hdf5\"\n", "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n", "    EarlyStop = EarlyStopping(monitor='val_acc', min_delta=0, patience=40, restore_best_weights=True, verbose=1,\n", "                              mode='max')\n", "    reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=10, cooldown=5, verbose=1, min_delta=0,\n", "                               mode='max', min_lr=0.0005)\n", "    callbacks_list = [EarlyStop, reduce, myCallback()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Training the model\n", "    history = model.fit([segment_train_images,segment_train_images_cat], segment_train_labels, validation_data = ([segment_validation_images,segment_validation_images_cat], segment_validation_labels), callbacks=callbacks_list, batch_size = 8, nb_epoch = 500, shuffle=True,verbose=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Finding Confusion Matrix using pretrained weights\n", "    predictions = model.predict([segment_validation_images,segment_validation_images_cat])\n", "    predictions_labels = numpy.argmax(predictions, axis=1)\n", "    validation_labels = numpy.argmax(segment_validation_labels, axis=1)\n", "    cfm = confusion_matrix(validation_labels, predictions_labels)\n", "    print (cfm)\n", "    print(\"accuracy: \",accuracy_score(validation_labels, predictions_labels))\n", "    return accuracy_score(validation_labels, predictions_labels), validation_labels, predictions_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-----------------------------------------------------------------------------------------------------------------<br>\n", "LOOCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loocv():\n", "    loo = LeaveOneOut()\n", "    loo.get_n_splits(segment_training_set)\n", "    tot = 0\n", "    count = 0\n", "    accs = []\n", "    accs2 = []\n", "    val_labels = []\n", "    pred_labels = []\n", "    for train_index, test_index in loo.split(segment_training_set):\n", "        # print(segment_traininglabels[train_index])\n", "        # print(segment_traininglabels[test_index])\n", "        print(test_index)\n", "        val_acc, val_label, pred_label = evaluate(segment_training_set[train_index], segment_training_set[test_index],\n", "                                                  segment_traininglabels[train_index],\n", "                                                  segment_traininglabels[test_index],\n", "                                                  test_index, segment_training_set_cat[train_index],\n", "                                                  segment_training_set_cat[test_index]\n", "                                                  )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        tot += val_acc\n", "        val_labels.extend(val_label)\n", "        pred_labels.extend(pred_label)\n", "        accs.append(val_acc)\n", "        accs2.append(segment_traininglabels[test_index])\n", "        count += 1\n", "        print(\"------------------------------------------------------------------------\")\n", "        print(\"validation acc:\", val_acc)\n", "        print(\"------------------------------------------------------------------------\")\n", "    print(\"accuracy: \", accuracy_score(val_labels, pred_labels))\n", "    cfm = confusion_matrix(val_labels, pred_labels)\n", "    # tp_and_fn = sum(cfm.sum(1))\n", "    # tp_and_fp = sum(cfm.sum(0))\n", "    # tp = sum(cfm.diagonal())\n", "    print(\"cfm: \\n\", cfm)\n", "    # print(\"tp_and_fn: \",tp_and_fn)\n", "    # print(\"tp_and_fp: \",tp_and_fp)\n", "    # print(\"tp: \",tp)\n", "    #\n", "    # precision = tp / tp_and_fp\n", "    # recall = tp / tp_and_fn\n", "    # print(\"precision: \",precision)\n", "    # print(\"recall: \",recall)\n", "    # print(\"F1-score: \",f1_score(val_labels,pred_labels,average=\"macro\"))\n", "    print(\"F1-score: \", f1_score(val_labels, pred_labels, average=\"weighted\"))\n", "    return val_labels, pred_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-----------------------------------------------------------------------------------------------------------------<br>\n", "Test train split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split():\n", "    # Spliting the dataset into training and validation sets\n", "    segment_train_images, segment_validation_images, segment_train_labels, segment_validation_labels = train_test_split(\n", "        segment_training_set,\n", "        segment_traininglabels,\n", "        test_size=0.2, random_state=42)\n\n", "    # Save validation set in a numpy array\n", "    # numpy.save('numpy_validation_datasets/{0}_images_{1}x{2}.npy'.format(segmentName,sizeH, sizeV), segment_validation_images)\n", "    # numpy.save('numpy_validation_datasets/{0}_images_{1}x{2}.npy'.format(segmentName,sizeH, sizeV), segment_validation_labels)\n\n", "    # Loading Load validation set from numpy array\n", "    #\n", "    # eimg = numpy.load('numpy_validation_datasets/{0}_images_{1}x{2}.npy'.format(segmentName,sizeH, sizeV))\n", "    # labels = numpy.load('numpy_validation_datasets/{0}_images_{1}x{2}.npy'.format(segmentName,sizeH, sizeV))\n", "    _, val_labels, pred_labels = evaluate(segment_train_images, segment_validation_images, segment_train_labels,\n", "                                          segment_validation_labels, 0)\n", "    return val_labels, pred_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-----------------------------------------------------------------------------------------------------------------------------<br>\n", "k-fold(10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kfold():\n", "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n", "    # kf.get_n_splits(segment_training_set)\n", "    tot = 0\n", "    count = 0\n", "    accs = []\n", "    accs2 = []\n", "    val_labels = []\n", "    pred_labels = []\n", "    for train_index, test_index in kf.split(segment_training_set):\n", "        # print(segment_traininglabels[train_index])\n", "        # print(segment_traininglabels[test_index])\n", "        print(test_index)\n", "        val_acc, val_label, pred_label = evaluate(segment_training_set[train_index], segment_training_set[test_index],\n", "                                                  segment_traininglabels[train_index],\n", "                                                  segment_traininglabels[test_index],\n", "                                                  test_index,segment_training_set_cat[train_index],segment_training_set_cat[test_index]\n", "                                                  )\n", "        tot += val_acc\n", "        val_labels.extend(val_label)\n", "        pred_labels.extend(pred_label)\n", "        accs.append(val_acc)\n", "        accs2.append(segment_traininglabels[test_index])\n", "        count += 1\n", "        print(\"------------------------------------------------------------------------\")\n", "        print(\"validation acc:\", val_acc)\n", "        print(\"------------------------------------------------------------------------\")\n", "    print(\"accuracy: \", accuracy_score(val_labels, pred_labels))\n", "    cfm = confusion_matrix(val_labels, pred_labels)\n", "    # tp_and_fn = sum(cfm.sum(1))\n", "    # tp_and_fp = sum(cfm.sum(0))\n", "    # tp = sum(cfm.diagonal())\n", "    print(\"cfm: \\n\", cfm)\n", "    # print(\"tp_and_fn: \",tp_and_fn)\n", "    # print(\"tp_and_fp: \",tp_and_fp)\n", "    # print(\"tp: \",tp)\n", "    #\n", "    # precision = tp / tp_and_fp\n", "    # recall = tp / tp_and_fn\n", "    # print(\"precision: \",precision)\n", "    # print(\"recall: \",recall)\n", "    # print(\"F1-score: \",f1_score(val_labels,pred_labels,average=\"macro\"))\n", "    print(\"F1-score: \", f1_score(val_labels, pred_labels, average=\"weighted\"))\n", "    return val_labels, pred_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##################################<br>\n", "edit params"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["K.set_image_dim_ordering('th')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segmentName = 'UpperFace_SelectiveDivideAndConquer_NEW'\n", "sizeH = 32\n", "sizeV = 32\n", "sizeD = 2\n", "segmentName2 = 'UpperFace_cat'\n", "sizeH2 = 32\n", "sizeV2 = 32\n", "sizeD2 = 30\n", "testtype = \"loocv\"\n", "###################################\n", "notes=\"8\"\n", "####################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load training images and labels that are stored in numpy array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segment_training_set = numpy.load(\n", "    'numpy_training_datasets/{0}_images_{1}x{2}x{3}.npy'.format(segmentName, sizeH, sizeV, sizeD))\n", "segment_traininglabels = numpy.load(\n", "    'numpy_training_datasets/{0}_labels_{1}x{2}x{3}.npy'.format(segmentName, sizeH, sizeV, sizeD))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segment_training_set_cat = numpy.load(\n", "    'numpy_training_datasets/{0}_images_{1}x{2}x{3}.npy'.format(segmentName2, sizeH2, sizeV2, sizeD2))\n", "# segment_traininglabels_cat = numpy.load(\n", "#     'numpy_training_datasets/{0}_labels_{1}x{2}x{3}.npy'.format(segmentName2, sizeH2, sizeV2, sizeD2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(segment_traininglabels)<br>\n", "print(numpy.sum(segment_traininglabels,axis=0))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if testtype == \"kfold\":\n", "    val_labels, pred_labels = kfold()\n", "elif testtype == \"loocv\":\n", "    val_labels, pred_labels = loocv()\n", "elif testtype == \"split\":\n", "    val_labels, pred_labels = split()\n", "else:\n", "    print(\"error\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------------------------------------------------------------------------------------------<br>\n", "write to results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = open(\"../TempResults.txt\", 'a')\n", "results.write(\"---------------------------\\n\")\n", "full_path = os.path.realpath(__file__)\n", "results.write(\n", "    str(os.path.dirname(full_path)) + \" {0}_{1}_{2}x{3}x{4}   {5}\\n\".format(testtype, segmentName, sizeH, sizeV, sizeD,notes))\n", "results.write(\"---------------------------\\n\")\n", "results.write(\"accuracy: \" + str(accuracy_score(val_labels, pred_labels)) + \"\\n\")\n", "results.write(\"F1-score: \" + str(f1_score(val_labels, pred_labels, average=\"weighted\")) + \"\\n\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}